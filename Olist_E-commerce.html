<!DOCTYPE html>
<html lang="en">
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>English Premier League</title>
    <!-- Prism.js CSS for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.css" rel="stylesheet" />
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            line-height: 1.6;
        }
        .media-element {
            display: block;
            margin: 20px auto;
            width: 60%; /* Fixed width */
            height: auto; /* Maintain aspect ratio */
        }
        
        .image-container .image-45 {
        width: 45%;
        height: auto;
        }
        
        .back-button {
            display: inline-block;
            padding: 10px 20px; /* Fixed padding */
            background-color: #007bff;
            color: white;
            text-decoration: none;
            border: none;
            border-radius: 5px;
            cursor: pointer;
        }
        p {
            color: #333; /* Dark gray for readability */
            font-size: 18px; /* Readable font size */
            margin: 20px;
            text-align: justify; /* Justify text for better presentation */
        }
    /* Custom styles for code block */
    pre {
      background-color: #f5f5f5; /* Lighter background */
      color: #2d2d2d; /* Dark text for contrast */
      padding: 20px;
      border-radius: 8px;
      font-family: 'Courier New', monospace;
      overflow-x: auto;
      max-width: 100%;
    }
    code {
      font-size: 1.1em;
    }

    .custom-list {
      padding-left: 30px; /* Adds space to the entire list */
    }

    .custom-list li {
      margin-left: 15px; /* Adds space to the text after the bullet */
    }
       
    </style>
  
</head>
<body>
    <a href="javascript:history.back()" class="back-button">Back</a>
    <p>
        Cloud computing plays a pivotal role in big data processing. In this context, I utilized the Azure ecosystem to prepare data for analysis. Initially, data was ingested into Azure Data Lake Storage Gen2 (ADLS Gen2). Subsequently, Azure Data Factory facilitated the automation of data pipelines, orchestrating the movement and transformation of data. The data was then processed in Azure Databricks for transformation tasks before being loaded into Azure Synapse Analytics for further analysis. Following the Medallion Architecture—a design pattern that organizes data into Bronze (raw), Silver (cleaned), and Gold (curated) layers—we extracted a refined subset of data and stored it back in ADLS Gen2 for analysis.
    </p>
    <img class="media-element" src="assets/Azure Architecture.png" alt="EPL Logo">
    <h1>Data Utilized</h1>
    <p>This is a Brazilian ecommerce public dataset of orders made at Olist Store. The dataset has information of 100k orders from 2016 to 2018 made at multiple marketplaces in Brazil. Its features allows viewing an order from multiple dimensions: from order status, price, payment and freight performance to customer location, product attributes and finally reviews written by customers. We also released a geolocation dataset that relates Brazilian zip codes to lat/lng coordinates.</p>
    <img class="media-element" src="assets/schema.png" alt="EPL Logo">
    <p style="text-align: center;"><b>The Database Schema for Olist E-commerce Data</b></p>
    <p>For making the project more interesting we have used multiple data sources. Here we have created MySQL database with the dataset olist_order_payments which will be later migrated to ADLS Gen 2 silver directory with data pipeline.</p>
    <h3>&nbsp;&nbsp;&nbsp;&nbsp;Information of olist_order_payments</h3>
    <ul>
    <li>order_id VARCHAR(255)(Primary_key)</li>
    <li>payment_sequential INT</li>
    <li> payment_type VARCHAR(255)</li>
    <li> payment_installments INT</li>
    <li>payment_value FLOAT</li>
    </ul>
    <div class="image-container">
    <img src="assets/dash.filess.io_(High Res).png" alt="Image_1" class="image-45" >
    <img src="assets/client.filess.io__connectionId=91d4371e-7aff-441c-9903-d6a086c100c5(High Res).png" alt="Image_2" class="image-45" >
    <p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><strong>MySQL Database</strong><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><strong>The Web Client for olist_order_payments_dataset</strong></p>
    </div>
    <p>This one is the severless MongoDB database which will be migrated to ADLS Gen 2 silver directory with Databricks.</p>
    <h3>&nbsp;&nbsp;&nbsp;&nbsp;Information of product categories</h3>
    <ul>
    <li>product_category_name VARCHAR(255)(Primary_key)</li>
    <li>product_category_name_english VARCHAR(255)</li>
    
    <div class="image-container">
    <img src="assets/mongo.png" alt="Image_1" class="image-45" >
    <img src="assets/client.filess.io__connectionId=8875de2c-86e8-4be9-9bff-6a6c6ea6cf4e(High Res).png" alt="Image_2" class="image-45" >
    <p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><strong>MongoDB Database</strong><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><strong>The Web Client for product_categories</strong></p>
    </div>
    
    <h1>Data Ingestion Pipeline with Azure Data Factory</h1>
    <img class="media-element" src="assets/Data piplining.png" alt="EPL Logo">
     <p style="text-align: center;"><b>The Data pipeline for migrating data from Github and MySQL Database</b></p>
    <h2>1. Copy Data Activity for GitHub to ADLS Gen2</h2>
    <ul>
        <li><strong>Activity Name:</strong> DatafromGithub</li>
        <li><strong>Source Dataset:</strong>
            <ul>
                <li><strong>Service:</strong> HTTP</li>
                <li><strong>File Format:</strong> CSV</li>
                <li><strong>Linked Service Configuration:</strong>
                    <ul>
                        <li><strong>Base URL:</strong> Location of GitHub files</li>
                        <li><strong>Authentication Type:</strong> Anonymous</li>
                        <li><strong>Parameterization:</strong> Dynamic content with <code>@dataset.csv_relative_url</code></li>
                    </ul>
                </li>
            </ul>
        </li>
        <li><strong>Sink Dataset:</strong>
            <ul>
                <li>Linked to ADLS Gen2 with dynamic parameter <code>@dataset.filename</code>.</li>
            </ul>
        </li>
    </ul>
    <h2>2. Copy Data Activity for MySQL to ADLS Gen2</h2>
    <ul>
        <li><strong>Source Dataset and Linked Service:</strong>
            <ul>
                <li>Dataset Name: <code>filessSQLBD</code></li>
                <li>Linked Service: <code>SQLtoADLSlinkedservice</code></li>
                <li>No parameterization needed for a single dataset.</li>
            </ul>
        </li>
    </ul>
    <h2>3. ForEach Activity for Iteration</h2>
    <ul>
        <li><strong>Purpose:</strong> Handle multiple files dynamically.</li>
        <li><strong>Dynamic Expressions:</strong>
            <ul>
                <li><strong>Source:</strong> <code>@item.csv_relative_url</code></li>
                <li><strong>Sink:</strong> <code>@item.filename</code></li>
            </ul>
        </li>
        <li>One important note is its always good to create a <strong>Copy Data Activity</strong> 
 and perform the configuarations most importantly the parameterizations before working with <code>ForEach activity</code>.</li>
    </ul>
    <h2>4. Lookup Activity for Automation</h2>
    <ul>
        <li><strong>Purpose:</strong> Automate file ingestion via JSON configuration.</li>
        <li><strong>Dataset and Linked Service:</strong>
            <ul>
                <li>Dataset Name: <code>ForEachjson</code></li>
                <li>Format: JSON</li>
                <li>Linked Service: <code>JsonfromGitHubForLoop</code></li>
            </ul>
        </li>
        <li><strong>Dynamic Expression in ForEach:</strong> <code>@activity('LookupForEachInput').output.value</code></li>
    </ul>
    <img class="media-element" src="assets/ForEach.png" alt="EPL Logo">
    <p style="text-align: center;"><b>ForEach file in Github to automate the ingestion process of multiple files in Data pipeline</b></p>

    <h2>5. Automation and Scalability</h2>
    <ul>
        <li>Update <code>ForEachInput.json</code> to add new CSV files seamlessly.</li>
    </ul>
    <h3>Key Notes:</h3>
    <ul>
        <li>Test configurations before adding them into <code>ForEach Activity</code>.</li>
        <li>Parameterization ensures flexibility and scalability.</li>
    </ul>
    <img class="media-element" src="assets/bronze.png" alt="EPL Logo">
    <p style="text-align: center;"><b>CSV files migrated to ADLS Gen 2 after running the data pipeline in Date Factory(Bronze Directory)</b></p>
    <h1>Data Transformation in Databricks and Storing the data in Silver Directory </h1>
   <img src="assets/Databricks Connector.png" alt="EPL Logo" 
     style="display: block; width: 90%; height: auto; border: 1px solid black; border-radius: 8px; margin: 0 auto;">
    <p><code>fs.azure.account.key.<storage-account-name>.dfs.core.windows.net</code> is the  statement sets the account key for authenticating access to an Azure Data Lake Storage Gen2 account. <code>dbutils.fs.ls</code> lists the files and directories present in the bronze container of the Azure Data Lake.</p>
        
    <img src="assets/Dataframes.png" alt="EPL Logo" 
     style="display: block; width: 90%; height: auto; border: 1px solid black; border-radius: 8px; margin: 0 auto;">
    <p><code>base_path:</code> Specifies the base path for accessing files in an Azure Data Lake Storage (ADLS) container named <code>olistdata</code>. The container is part of the storage account <code>olistecommstorageaccnt</code>. These variables(<code>order_path</code>...<code>products_path</code>) define the full paths to individual datasets stored in the bronze layer of the data lake. Each dataset is a CSV file and contains specific data related to the Olist e-commerce platform. </p>
    <p><code>spark.read.format("csv"):</code> Specifies that the data format to be read is CSV. <code>.option("header", "true")</code>: Indicates that the CSV files contain a header row, and column names should be inferred from it.<code>.load(order_path)</code>: Loads the dataset from the specified order_path. The same operation is performed for other datasets.</p>
    <img src="assets/Mongodb.png" alt="EPL Logo" 
     style="display: block; width: 90%; height: auto; border: 1px solid black; border-radius: 8px; margin: 0 auto;">
     <p>This code connects to a MongoDB server using authentication credentials, accesses the database <code>OlistNoSQLDatabase_imaginecat</code>, retrieves the <code>product_categories</code> collection and loads the collection data into a Pandas DataFrame for analysis.</p>   
    <img src="assets/Joins.png" alt="EPL Logo" 
     style="display: block; width: 90%; height: auto; border: 1px solid black; border-radius: 8px; margin: 0 auto;">
        

    <img src="assets/Dropping duplicates.png" alt="EPL Logo" 
    style="display: block; width: 90%; height: auto; border: 1px solid black; border-radius: 8px; margin: 0 auto;">
        
    <h1>Data Integration through Azure Synapse Analytics and Storing the data in Gold Directory </h1>
   <pre><code class="language-sql">
create schema gold      
create view gold.final
as
SELECT 
    *
FROM 
    OPENROWSET(
          BULK 'https://olistecommstorageaccnt.dfs.core.windows.net/olistdata/silver',
          FORMAT = 'PARQUET') 
          AS result1
where order_status = 'delivered'
       
select * from  gold.final
       
  </code></pre>
 <pre><code class="language-sql">
---CREATE MASTER KEY ENCRYPTION BY PASSWORD = 'V3gaDreamy007';
---CREATE DATABASE SCOPED CREDENTIAL Md_Sajid WITH IDENTITY = 'Managed Identity';

CREATE EXTERNAL FILE FORMAT exfileformat WITH (
    FORMAT_TYPE = PARQUET,
    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'
);

CREATE EXTERNAL DATA SOURCE goldlayer WITH (
    LOCATION = 'https://olistecommstorageaccnt.dfs.core.windows.net/olistdata/gold/',
    CREDENTIAL = Md_Sajid
);

CREATE EXTERNAL TABLE gold.finaltable  WITH (
        LOCATION = Serving',
        DATA_SOURCE = goldlayer,
        FILE_FORMAT = exfileformat
) AS
SELECT * FROM gold.final;
  </code></pre>

  
  <!-- Include Prism.js for syntax highlighting -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
  <!-- Include the SQL language definition -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-sql.min.js"></script>
  
  <script>
    // Initialize Prism to apply syntax highlighting
    Prism.highlightAll();
  </script>
    
</body>
</html>

