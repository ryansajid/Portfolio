<!DOCTYPE html>
<html lang="en">
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>English Premier League</title>
    
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            line-height: 1.6;
        }
        .media-element {
            display: block;
            margin: 20px auto;
            width: 60%; /* Fixed width */
            height: auto; /* Maintain aspect ratio */
        }
        
        .image-container .image-45 {
        width: 45%;
        height: auto;
        }
        
        .back-button {
            display: inline-block;
            padding: 10px 20px; /* Fixed padding */
            background-color: #007bff;
            color: white;
            text-decoration: none;
            border: none;
            border-radius: 5px;
            cursor: pointer;
        }
        p {
            color: #333; /* Dark gray for readability */
            font-size: 18px; /* Readable font size */
            margin: 20px;
            text-align: justify; /* Justify text for better presentation */
        }
       
       
    </style>
  
</head>
<body>
    <a href="javascript:history.back()" class="back-button">Back</a>
    <p>
        Cloud computing plays a pivotal role in big data processing. In this context, I utilized the Azure ecosystem to prepare data for analysis. Initially, data was ingested into Azure Data Lake Storage Gen2 (ADLS Gen2). Subsequently, Azure Data Factory facilitated the automation of data pipelines, orchestrating the movement and transformation of data. The data was then processed in Azure Databricks for transformation tasks before being loaded into Azure Synapse Analytics for further analysis. Following the Medallion Architecture—a design pattern that organizes data into Bronze (raw), Silver (cleaned), and Gold (curated) layers—we extracted a refined subset of data and stored it back in ADLS Gen2 for analysis.
    </p>
    <img class="media-element" src="assets/Azure Architecture.png" alt="EPL Logo">
    <h1>Data Utilized</h1>
    <p>This is a Brazilian ecommerce public dataset of orders made at Olist Store. The dataset has information of 100k orders from 2016 to 2018 made at multiple marketplaces in Brazil. Its features allows viewing an order from multiple dimensions: from order status, price, payment and freight performance to customer location, product attributes and finally reviews written by customers. We also released a geolocation dataset that relates Brazilian zip codes to lat/lng coordinates.</p>
    <img class="media-element" src="assets/schema.png" alt="EPL Logo">
    <p style="text-align: center;"><b>The Database Schema for Olist E-commerce Data</b></p>
    <div class="image-container">
    <img src="assets/dash.filess.io_(High Res).png" alt="Image_1" class="image-45" >
    <img src="assets/client.filess.io__connectionId=91d4371e-7aff-441c-9903-d6a086c100c5(High Res).png" alt="Image_2" class="image-45" >
    <p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><strong>MySQL Database</strong><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><strong>The Web Client for olist_order_payments_dataset</strong></p>
    </div>

     <div class="image-container">
    <img src="assets/mongo.png" alt="Image_1" class="image-45" >
    <img src="assets/client.filess.io__connectionId=8875de2c-86e8-4be9-9bff-6a6c6ea6cf4e(High Res).png" alt="Image_2" class="image-45" >
    <p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><strong>MongoDB Database</strong><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><strong>The Web Client for product_categories</strong></p>
    </div>
    
    <h1>Data Ingestion Pipeline with Azure Data Factory</h1>
    <img class="media-element" src="assets/Data piplining.png" alt="EPL Logo">
     <p style="text-align: center;"><b>The Data pipeline for migrating data from Github and MySQL Database</b></p>
    <h2>1. Copy Data Activity for GitHub to ADLS Gen2</h2>
    <ul>
        <li><strong>Activity Name:</strong> DatafromGithub</li>
        <li><strong>Source Dataset:</strong>
            <ul>
                <li><strong>Service:</strong> HTTP</li>
                <li><strong>File Format:</strong> CSV</li>
                <li><strong>Linked Service Configuration:</strong>
                    <ul>
                        <li><strong>Base URL:</strong> Location of GitHub files</li>
                        <li><strong>Authentication Type:</strong> Anonymous</li>
                        <li><strong>Parameterization:</strong> Dynamic content with <code>@dataset.csv_relative_url</code></li>
                    </ul>
                </li>
            </ul>
        </li>
        <li><strong>Sink Dataset:</strong>
            <ul>
                <li>Linked to ADLS Gen2 with dynamic parameter <code>@dataset.filename</code>.</li>
            </ul>
        </li>
    </ul>
    <h2>2. Copy Data Activity for MySQL to ADLS Gen2</h2>
    <ul>
        <li><strong>Source Dataset and Linked Service:</strong>
            <ul>
                <li>Dataset Name: <code>filessSQLBD</code></li>
                <li>Linked Service: <code>SQLtoADLSlinkedservice</code></li>
                <li>No parameterization needed for a single dataset.</li>
            </ul>
        </li>
    </ul>
    <h2>3. ForEach Activity for Iteration</h2>
    <ul>
        <li><strong>Purpose:</strong> Handle multiple files dynamically.</li>
        <li><strong>Dynamic Expressions:</strong>
            <ul>
                <li><strong>Source:</strong> <code>@item.csv_relative_url</code></li>
                <li><strong>Sink:</strong> <code>@item.filename</code></li>
            </ul>
        </li>
        <li>One important note is its always good to create a <strong>Copy Data Activity</strong> 
 and perform the configuarations most importantly the parameterizations before working with <code>ForEach activity</code>.</li>
    </ul>
    <h2>4. Lookup Activity for Automation</h2>
    <ul>
        <li><strong>Purpose:</strong> Automate file ingestion via JSON configuration.</li>
        <li><strong>Dataset and Linked Service:</strong>
            <ul>
                <li>Dataset Name: <code>ForEachjson</code></li>
                <li>Format: JSON</li>
                <li>Linked Service: <code>JsonfromGitHubForLoop</code></li>
            </ul>
        </li>
        <li><strong>Dynamic Expression in ForEach:</strong> <code>@activity('LookupForEachInput').output.value</code></li>
    </ul>
    <img class="media-element" src="assets/ForEach.png" alt="EPL Logo">
    <p style="text-align: center;"><b>ForEach file in Github to automate the ingestion process of multiple files in Data pipeline</b></p>

    <h2>5. Automation and Scalability</h2>
    <ul>
        <li>Update <code>ForEachInput.json</code> to add new CSV files seamlessly.</li>
    </ul>
    <h3>Key Notes:</h3>
    <ul>
        <li>Test configurations before adding them into <code>ForEach Activity</code>.</li>
        <li>Parameterization ensures flexibility and scalability.</li>
    </ul>
    <img class="media-element" src="assets/bronze.png" alt="EPL Logo">
    <p style="text-align: center;"><b>CSV files migrated to ADLS Gen 2 after running the data pipeline in Date Factory(Bronze Directory)</b></p>
    <h1>Data Transformation in Databricks</h1>
   

</body>
</html>

